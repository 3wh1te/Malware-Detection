# coding=utf-8
# from tensorflow.keras.preprocessing.text import Tokenizer
import os.path as osp
from torch_geometric.io import read_tu_data
import os
import json
import numpy as np
import torch
from torch_geometric.data import InMemoryDataset
import pandas as pd
from jgraph import *
import networkx
import tqdm
import collections
'''
todo: 如何将一个描述图的gdl转化成邻接矩阵A、度矩阵D、特征矩阵X？
todo: 不同的图的点的个数是不同的，那么矩阵的shape如何确定，也就是n的值是多少？不需要统一
'''


class MyDataset(InMemoryDataset):
    def __init__(self, root, name, transform=None, pre_transform=None,
                 pre_filter=None, use_node_attr=False, use_edge_attr=False,
                 cleaned=False):
        self.name = name
        self.cleaned = cleaned
        super(MyDataset, self).__init__(root, transform, pre_transform,
                                        pre_filter)
        self.data, self.slices = torch.load(self.processed_paths[0])
        if self.data.x is not None and not use_node_attr:
            num_node_attributes = self.num_node_attributes
            self.data.x = self.data.x[:, num_node_attributes:]
        if self.data.edge_attr is not None and not use_edge_attr:
            num_edge_attributes = self.num_edge_attributes
            self.data.edge_attr = self.data.edge_attr[:, num_edge_attributes:]

    @property
    def raw_dir(self):
        name = 'raw{}'.format('_cleaned' if self.cleaned else '')
        return osp.join(self.root, self.name, name)

    @property
    def processed_dir(self):
        name = 'processed{}'.format('_cleaned' if self.cleaned else '')
        return osp.join(self.root, self.name, name)

    @property
    def num_node_labels(self):
        if self.data.x is None:
            return 0
        for i in range(self.data.x.size(1)):
            x = self.data.x[:, i:]
            if ((x == 0) | (x == 1)).all() and (x.sum(dim=1) == 1).all():
                return self.data.x.size(1) - i
        return 0

    @property
    def num_node_attributes(self):
        if self.data.x is None:
            return 0
        return self.data.x.size(1) - self.num_node_labels

    @property
    def num_edge_labels(self):
        if self.data.edge_attr is None:
            return 0
        for i in range(self.data.edge_attr.size(1)):
            if self.data.edge_attr[:, i:].sum() == self.data.edge_attr.size(0):
                return self.data.edge_attr.size(1) - i
        return 0

    @property
    def num_edge_attributes(self):
        if self.data.edge_attr is None:
            return 0
        return self.data.edge_attr.size(1) - self.num_edge_labels

    @property
    def raw_file_names(self):
        names = ['A', 'graph_indicator']
        return ['{}_{}.txt'.format(self.name, name) for name in names]

    @property
    def processed_file_names(self):
        return 'data.pt'

    def download(self):
        pass

    def process(self):
        self.data, self.slices = read_tu_data(self.raw_dir, self.name)

        if self.pre_filter is not None:
            data_list = [self.get(idx) for idx in range(len(self))]
            data_list = [data for data in data_list if self.pre_filter(data)]
            self.data, self.slices = self.collate(data_list)

        if self.pre_transform is not None:
            data_list = [self.get(idx) for idx in range(len(self))]
            data_list = [self.pre_transform(data) for data in data_list]
            self.data, self.slices = self.collate(data_list)

        torch.save((self.data, self.slices), self.processed_paths[0])

    def __repr__(self):
        return '{}({})'.format(self.name, len(self))


# 提取节点特征矩阵、标签、邻接矩阵
class Processor:
    def __init__(self, path, type):
        '''
        :param path: 文件路径
        :param type: 文件类型
        '''
        self.path = path
        self.type = type
        self.edge_index = []
        self.X = []
        self.Y = []
        self.labels = pd.read_csv('data/trainLabels.csv')
        self.labels.set_index('Id', inplace=True)


    def process(self):
        files = os.listdir(self.path)
        if self.type == 'gdl':
            for gdl in tqdm.tqdm(files):
                A, node_attr = self.process_gdl('{0}/{1}'.format(self.path, gdl))
                self.X.append(node_attr)
                self.edge_index.append(A)
                self.Y.append(self.labels.loc[gdl.split('.')[0], 'Class'])
        if self.type == 'gml':
            for gml in tqdm.tqdm(files):
                try:
                    A, node_attr = self.process_gml('{0}/{1}'.format(self.path, gml))
                except Exception:
                    print(gml)
                    continue
                self.X.append(node_attr)
                self.edge_index.append(A)
                self.Y.append(self.labels.loc[gml.split('.')[0], 'Class'])

    def process_gml(self, gml_file):
        node_attr = []
        G = networkx.read_gml(gml_file, label='id')
        # g = Graph.Load(gml_file, format='gml')
        A = list(G.edges)
        for node in G.nodes:
            node_attr.append(G.nodes[node])
        return A, node_attr

    def process_gdl(self, gdl_file):
        A = []
        node_attr = []

        with open(gdl_file, 'r') as gdl:
            data = gdl.readlines()
        for line in data:
            if line.startswith('node'):
                node = line.split('node:')[1].split(' ')
                node_attr.append(eval(node[5]))
            if line.startswith('edge'):
                edge = line.split('edge:')[1].split(' ')
                A.append((eval(edge[3]), eval(edge[5])))
        return A, node_attr

    def get_data(self):
        return self.edge_index, self.X, self.Y

    #  将数据存入文件当中
    def save_data(self):
        indicator = [1]
        with open('data/gml/raw/{}.txt'.format(self.type), 'w') as wgif:
            for index, g in enumerate(self.X):
                indicator.append(len(g))
                for _ in range(len(g)):
                    wgif.write('{}\n'.format(str(index + 1)))
        indicator = np.cumsum(np.array(indicator))

        with open('data/gml/raw/{}_A.txt'.format(self.type), 'w') as waf:
            for index, g in enumerate(self.edge_index):
                start = indicator[index]
                for edge in g:
                    source = int(edge[0])
                    target = int(edge[1])
                    waf.write('\t{0},{1}\n'.format(str(start + source), str(start + target)))
                    waf.write('\t{0},{1}\n'.format(str(start + target), str(start + source)))

        with open('data/gml/raw/{}_attr.txt'.format(self.type), 'w') as watf:
            for g in self.X:
                for node in g:
                    watf.write(json.dumps(node) + '\n')

        with open('data/gml/raw/{}_graph_labels.txt'.format(self.type), 'w') as wlbf:
            for y in self.Y:
                wlbf.write(str(y) + '\n')

    # app name
    def get_title(self, gdl):
        with open(gdl, 'r') as rf:
            return rf.readlines()[1]

    # 加载节点字典
    def load_dict(self, path):
        with open(path, 'r') as fp:
            self.fdict = json.load(fp)

    # 生成字典
    # def generate_dict(self):
    #     tokenizer = Tokenizer(split=' ')
    #     seq = [' '.join(x) for x in self.X]
    #     tokenizer.fit_on_texts(seq)
    #     return tokenizer.to_json()

    # 将字典保存到本地
    def save_dict(self, path):
        with open(path, 'w') as fp:
            json.dump(self.fdict, fp)


def process_node_attr(path):

    with open(path, 'r') as rf:
        node_attrs = rf.readlines()
        labels = []
        types = []
        inst_lens = []

    for node in node_attrs:
        attr = json.loads(node)
        labels.append(attr['label'])
        if type == 'Internal':
            types.append(1)
            inst_lens.append(len(attr['features']['val']))
        else:
            types.append(0)
            inst_lens.append(0)
    #
    print(len(labels))
    label_freq = collections.Counter(labels)
    sorted_labels = sorted(label_freq.items(), key=lambda x:x[1])
    # (label type, len)
    print(label_freq.most_common(500000))
    print(len(sorted_labels))
    labels_dict = {}
    flag = 0
    for index, label in enumerate(sorted_labels):
        if label[1] > 10:
            labels_dict[label[0]] = (index - flag + 1, types[index], inst_lens[index])
        else:
            flag += 1
            labels_dict[label[0]] = (0, types[index], inst_lens[index])
    with open('data/dict.json', 'w') as wf:
        json.dump(wf, labels_dict)
    # pd.DataFrame(labels_dict)
    # f = pd.DataFrame(label_freq.most_common(), columns=['Label', 'type', 'inst_len'])
    df = pd.DataFrame(label_freq.most_common(), columns=['Label', 'times'])
    df.to_csv('data/tem/node_attr.csv')



if __name__ == '__main__':
    # p = Processor('data/gdl_samples', 'gml')
    # p.save_dict('data/dict/dict.json')
    # p.save_data()
    # p = Processor('../gmls/', 'gml')
    # p.process()
    # print(p.Y)
    # a, x = p.process_one('data/gdl_samples/test.gml')
    # print(p.edge_index)
    # print(p.X)
    # print(p.Y)
    # p.save_data()
    process_node_attr('data/tem/gml_attr.txt')

