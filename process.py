# coding=utf-8
import glob
import re
from tensorflow.keras.preprocessing.text import Tokenizer
# from sklearn.feature_extraction.text import *
# import angr
import os
import json
import numpy as np
import torch
from torch_geometric.data import InMemoryDataset
from tqdm import tqdm
import pandas as pd
'''
todo: 如何将一个描述图的gdl转化成邻接矩阵A、度矩阵D、特征矩阵X？
todo: 不同的图的点的个数是不同的，那么矩阵的shape如何确定，也就是n的值是多少？不需要统一
'''



class MyDataset(InMemoryDataset):
    def __init__(self, root, transform=None, pre_transform=None):
        super(MyDataset, self).__init__(root, transform, pre_transform)
        self.data, self.slices = torch.load(self.processed_paths[0])

    @property
    def raw_file_names(self):
        return []

    @property
    def processed_file_names(self):
        return ['../input/yoochoose_click_binary_1M_sess.dataset']

    def download(self):
        pass

    def process(self):
        pass


# 提取节点特征矩阵、标签、邻接矩阵
class Processor:
    def __init__(self, path, type):
        '''
        :param path: 文件路径
        :param type: 文件类型
        '''
        self.path = path
        self.type = type
        self.edge_index = []
        self.X = []
        self.Y = []
        self.process()
        self.fdict = self.generate_dict()

    def process(self):
        gdls = os.listdir(self.path)
        for gdl in gdls:
            A, node_attr = self.process_one('{0}/{1}'.format(self.path, gdl))
            self.X.append(node_attr)
            self.edge_index.append(A)
            self.Y.append(gdl.split('.')[0])

    def process_one(self, gdl_file):
        A = []
        node_attr = []
        with open(gdl_file, 'r') as gdl:
            data = gdl.readlines()
        for line in data:
            if line.startswith('node'):
                node = line.split('node:')[1].split(' ')
                node_attr.append(eval(node[5]))
            if line.startswith('edge'):
                edge = line.split('edge:')[1].split(' ')
                A.append((eval(edge[3]), eval(edge[5])))
        return A, node_attr

    def get_data(self):
        return self.edge_index, self.X, self.Y

    #  将数据存入文件当中
    def save_data(self):
        indicator = [1]
        with open('data/processed/gdl_gindicator.txt', 'w') as wgif:
            for index, g in enumerate(self.X):
                indicator.append(len(g))
                for _ in range(len(g)):
                    wgif.write('{}\n'.format(str(index + 1)))
        indicator = np.cumsum(np.array(indicator))

        with open('data/processed/gdl_A.txt', 'w') as waf:
            for index, g in enumerate(self.edge_index):
                start = indicator[index]
                for edge in g:
                    source = int(edge[0])
                    target = int(edge[1])
                    waf.write('\t{0},{1}\n'.format(str(start + source), str(start + target)))
                    waf.write('\t{0},{1}\n'.format(str(start + target), str(start + source)))

        with open('data/processed/gdl_attr.txt', 'w') as watf:
            for g in self.X:
                for node in g:
                    watf.write(node + '\n')

        with open('data/processed/gdl_label.txt', 'w') as wlbf:
            for y in self.Y:
                wlbf.write(y + '\n')

    # app name
    def get_title(self, gdl):
        with open(gdl, 'r') as rf:
            return rf.readlines()[1]

    # 加载节点字典
    def load_dict(self, path):
        with open(path, 'r') as fp:
            self.fdict = json.load(fp)

    # 生成字典
    def generate_dict(self):
        tokenizer = Tokenizer(split=' ')
        seq = [' '.join(x) for x in self.X]
        tokenizer.fit_on_texts(seq)
        return tokenizer.to_json()

    # 将字典保存到本地
    def save_dict(self, path):
        with open(path, 'w') as fp:
            json.dump(self.fdict, fp)

# def gen_call_graph(path='data/test/reverse.exe'):
#     project = angr.Project(path)
#     cfg = project.analyses.CFG(show_progressbar=True)
#     print(cfg.functions.callgraph())
#
#
# def load_files_from_dir(dir):
#     files = glob.glob(dir)
#     result = []
#     for file in files:
#         with open(file) as f:
#             lines = f.readlines()
#             lines_to_line = " ".join(lines)
#             lines_to_line = re.sub(r"[APT|Crypto|Locker|Zeus]", ' ', lines_to_line, flags=re.I)
#             result.append(lines_to_line)
#     return result
#
#
# def load_files(dir):
#     malware_class = ['APT', 'Crypto', 'Locker', 'Zeus']
#     x = []
#     y = []
#     for i, family in enumerate(malware_class):
#         dir = "../data/malware/MalwareTrainingSets-master/trainingSets/%s/*" % family
#         print("Load files from %s" % dir)
#         v = load_files_from_dir(dir)
#         x += v
#         y += [i] * len(v)
#     return x, y


if __name__ == '__main__':
    p = Processor('data/gdl_samples', 'gdl')
    p.save_dict('data/dict/dict.json')
    p.save_data()

    # a, x = p.process_one('data/gdl_samples/test.gdl')
    # print(p.edge_index)
    # print(p.X)
    # print(p.Y)
    # p.save_data()
